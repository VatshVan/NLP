{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import swifter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "text = df['review'].to_list()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, language='english'):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # 3. Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # 4. Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 5. Tokenize\n",
    "    word_tokens = text.split()\n",
    "\n",
    "    # 6. Remove stopwords\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    filtered_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "    # 7. Stemming\n",
    "    stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "    # 8. Lemmatization\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos='v') for word in stems]\n",
    "\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_cache(df, text_col='review', output_col='clean_text', cache_file='cleaned_texts.pkl'):\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading preprocessed texts from cache: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cleaned_texts = pickle.load(f)\n",
    "        if len(cleaned_texts) == len(df):\n",
    "            df[output_col] = cleaned_texts\n",
    "            print(f\"Loaded {len(cleaned_texts)} cached texts successfully.\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Cache size mismatch. Reprocessing from scratch...\")\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Starting preprocessing | Memory used: {mem.percent}% | Available: {round(mem.available / (1024**3), 2)} GB\")\n",
    "    df[output_col] = df[text_col].swifter.apply(text_preprocess)\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(df[output_col].tolist(), f)\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Preprocessing complete | Memory used: {mem.percent}% | Saved to {cache_file}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_with_cache(df)\n",
    "text = df['clean_text'].to_list()\n",
    "all_text = ' '.join(df['clean_text'].astype(str))\n",
    "tokens = word_tokenize(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "emoji_pattern = r'^(?:[\\u2700-\\u27bf]|(?:\\ud83c[\\udde6-\\uddff]){1,2}|(?:\\ud83d[\\udc00-\\ude4f]){1,2}|[\\ud800-\\udbff][\\udc00-\\udfff]|[\\u0021-\\u002f\\u003a-\\u0040\\u005b-\\u0060\\u007b-\\u007e]|\\u3299|\\u3297|\\u303d|\\u3030|\\u24c2|\\ud83c[\\udd70-\\udd71]|\\ud83c[\\udd7e-\\udd7f]|\\ud83c\\udd8e|\\ud83c[\\udd91-\\udd9a]|\\ud83c[\\udde6-\\uddff]|\\ud83c[\\ude01-\\ude02]|\\ud83c\\ude1a|\\ud83c\\ude2f|\\ud83c[\\ude32-\\ude3a]|\\ud83c[\\ude50-\\ude51]|\\u203c|\\u2049|\\u25aa|\\u25ab|\\u25b6|\\u25c0|\\u25fb|\\u25fc|\\u25fd|\\u25fe|\\u2600|\\u2601|\\u260e|\\u2611|[^\\u0000-\\u007F])+$'\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word.lower() not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(emoji_pattern, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# Combine all text into one string\n",
    "all_text = ' '.join(df['clean_text'].astype(str))\n",
    "\n",
    "# Split sentences using regex (avoids NLTK punkt)\n",
    "sentences_list = re.split(r'(?<=[.!?])\\s+', all_text)\n",
    "\n",
    "# Preprocess sentences\n",
    "corpus = [preprocess_sentence(sentence) for sentence in sentences_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_array = X.toarray()\n",
    "\n",
    "print(\"Unique Word List: \\n\", feature_names)\n",
    "print(\"Bag of Words Matrix: \\n\", X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=X_array, columns=feature_names, index=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df = preprocess_with_cache(df)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove numbers, punctuation, emojis\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(emoji_pattern, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize, remove stopwords, lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Combine all text into one list (optional, for sentence-level split)\n",
    "all_text = df['clean_text'].astype(str).tolist()\n",
    "\n",
    "# Process each row (fast, memory-efficient)\n",
    "tqdm.pandas()  # optional: adds progress bar\n",
    "df['processed_text'] = df['clean_text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Result: df['processed_text'] contains cleaned, lemmatized, stopword-free text\n",
    "print(df['processed_text'].head())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(df['processed_text'])\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "df1 = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = df['processed_text'].astype(str).tolist()\n",
    "tfidf_matrix = vectorizer.fit_transform(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vectorizer.vocabulary_)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c919fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the corpus using NLTK\n",
    "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Flatten the list to get all words in the corpus\n",
    "all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "\n",
    "# Get unique words (vocabulary)\n",
    "vocab = sorted(set(all_words))\n",
    "\n",
    "# Print vocabulary\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Reshape the list of words into a 2D array for OneHotEncoder\n",
    "word_array = np.array(all_words).reshape(-1, 1)\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=True)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(word_array)\n",
    "\n",
    "# Print the one-hot encoded data\n",
    "print(\"One-hot encoded matrix:\\n\", one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=0,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")\n",
    "cbow_model.save('cbow_model.model')\n",
    "skipgram_model.save('skipgram_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CBOW vocab size: {len(cbow_model.wv)}\")\n",
    "print(f\"Skip-Gram vocab size: {len(skipgram_model.wv)}\")\n",
    "\n",
    "if 'word2vec' in cbow_model.wv and 'gensim' in cbow_model.wv:\n",
    "    sim = cbow_model.wv.similarity('word2vec', 'gensim')\n",
    "    print(f\"CBOW similarity: {sim}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
